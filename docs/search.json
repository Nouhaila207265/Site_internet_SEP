[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Présentation du Master SEP",
    "section": "",
    "text": "Le Master Statistique pour l’Évaluations et Prévision (SEP) offre aux étudiants l’opportunité d’acquérir une formation sur deux ans dans les domaines de l’analyse économique quantitative et de l’aide à la décision. La dualité de cette formation, à la fois théorique et appliquée, permet d’acquérir des compétences reconnues tant dans le monde professionnel que dans le cadre de la préparation d’un doctorat.\nLe parcours type « Statistique pour l’Évaluation et Prévision » (SEP) forme des statisticiens économistes aptes au dialogue avec des non spécialistes, performants dans tout domaine statistique, sur tout support informatique, et particulièrement efficients sur les problématiques de l’évaluation économique et sociale, de la Data Science, du traitement de données massives (Big Data), du traitement de données marketing et de la gestion des risques.\nA l’issue de la formation SEP, les étudiants auront acquis et approfondi les compétences dans les domaines\n– quantitatifs : analyse des données, séries temporelles, apprentissage statistique, aspects de classification et de mise en place de scores, économétrie, géostatistique, data mining, modélisation mathématique, techniques quantitatives en évaluation, fondements des mathématiques financières, mesure des risques.\n– informatiques : maîtrise des logiciels dédiés à la statistique et à la modélisation tels que IBM-SPSS, SAS ou encore R et Python, qui sont assimilés en contexte grâce à leur utilisation systématique lors des enseignements de statistique et en mode projet. Les aspects de codage ne sont pas en reste : les macro sous Excel, la programmation en VB-VBA ainsi qu’en langage objet R et Python ou encore la syntaxe de SAS et d’IBM-SPSS, font l’objet d’un enseignement spécifique. Enfin les requêtes ACCESS et SQL Server sont elles aussi maîtrisées. Les logiciels Hadoop et Spark dédiés au Big Data sont également présentés ;\n– métiers : Data Science, évaluation, prospective, mesure et gestion des risques en particulier financiers, marketing, télécommunications, projets Big Data, économie du développement durable, bio-économie.\nLa formation est pluridisciplinaire : les enseignements sont dispensés par des économistes, des mathématiciens et des informaticiens, ainsi que des professionnels des secteurs d’applications.\nLes compétences acquises dans ce Master dépassent largement le cadre de la statistique appliquée et de l’économétrie, pour couvrir les champs de l’économie et de l’informatique.\nUn premier tiers des enseignements est dispensé par des probabilistes-statisticiens et informaticiens, un second par des économistes, le reste étant réalisé par des professionnels des méthodes quantitatives rompus aux demandes et restitutions « tout public ».\nNos futurs statisticiens sont confrontés, dès leur formation, à la multidisciplinarité et à la nécessité d’user d’un langage commun grâce au recrutement d’étudiants issus de cursus variés : scientifique (mathématiciens, probabilistes, ingénieurs pluridisciplinaires et informaticiens) et domaine tertiaire (économistes avec fort acquis mathématique, économètres, gestionnaires, géographes ou sociologues ayant de fortes appétences quantitatives). La première année du master, les enseignements sont pour les 6/10ème en rapport avec leur cursus d’origine pour aboutir à une offre commune en 2nde année.\nEn première année et dans une moindre mesure, en seconde année, mutualisation d’un certain nombre d’enseignements avec les parcours : « Transition écologique, politiques publiques» et « Entrepreneuriat, Innovation et Bio-économie » de la mention Économie Appliquée, «Calcul Scientifique» de la mention Mathématiques et Applications, « Calcul Haute Performance, Simulation ».\nLa formation s’achève avec un stage long soutenu devant l’ensemble du corps enseignant et de la nouvelle promotion. La pédagogie mise en œuvre est fondée sur la réalisation de projets utilisant les acquis de plusieurs enseignements, et réalisés systématiquement en groupe.\nLa formation se déroule sur le campus Sciences et sur le campus Économie de l’Université de Reims.\nLe Master délivre le SAS Joint Certificate Program « SAS Programing & Data Analysis »."
  },
  {
    "objectID": "Modalités.html",
    "href": "Modalités.html",
    "title": "Modalités d’inscription",
    "section": "",
    "text": "La spécialité S.E.P. est une spécialité partagée du Master Mathématiques et Applications de la Faculté des Sciences Exactes et Naturelles et du Master Analyse et Politique Économique de la Faculté des Sciences Économiques, Sociales et de Gestion.\nLa spécialité est accessible en première et 2nde année de master, sur dossier et après test et entretien, aux étudiants ayant validé une licence et/ou une première année de Master dans le domaine des mathématiques ou de l’informatique ou dans le domaine des sciences économiques ou de gestion. Elle est également ouverte aux titulaires d’un diplôme d’école de commerce ou d’ingénieur. De solides bases en probabilités et en statistique sont indispensables.\nLe diplôme est ouvert aux personnes désirant se former dans le domaine de la statistique appliquée à l’évaluation économique ou se réorienter professionnellement. Ces derniers peuvent alors être accueillis en formation continue et s’inscrire dans une démarche de VAE.\nAprès sélection aux différents niveaux et pour bien préparer ces deux années au mieux, il est fourni aux  étudiants un programme de préparation au master SEP appelé SepCamp concocté souvent par les étudiants de l’année en cours et de l’année précédente, un outil indispensable pour l’installation et la mise à jour de certains logiciels comme SAS, Python, R, SQL  et l’approfondissement de leurs connaissances en algèbre, probabilité et statistique (pour tous), et en économie. Voici le lien SepCamp.\nS’inscrire au parcours SEP :\nM1 mention Mathématiques et applications\nM1 mention Analyse et Politique Economique"
  },
  {
    "objectID": "plan_acces.html",
    "href": "plan_acces.html",
    "title": "Plan d’accès",
    "section": "",
    "text": "Les cours sont dispensés dans les deux campus : Campus Croix Rouge à la faculté d’économie et Campus Moulin de la Housse à la faculté des sciences, vous trouvez ci-dessous les adresses :\n\nUniversité de Reims Champagne-Ardenne :\nCampus Croix-Rouge\nBâtiment 18\n57 rue Pierre Taittinger\n51100 Reims\n\nCampus Moulin de la Housse\nDépartement Maths/Info\n16 Chemin des Rouliers\n51100 Reims\n\n\n\nVenir à Reims\nPar la route :\nÀ la croisée des autoroutes :\nA4 – E50 (Paris –Strasbourg-Allemagne)\nA26 – E17 (Lille –Lyon-Méditerranée)\nA34 – E46 (Ardennes –Belgique)\n6 sorties desservent la ville\nVous pouvez établir votre itinéraire en suivant les liens suivants :\nwww.mappy.com\nwww.viamichelin.com\nPar le train :\nReims est à 45’ de Paris et 30’ de Roissy Charles De Gaulle par le TGV Est Européen :\n2 gares TGV :\n* Gare TGV Reims centre : 8 A/R pour Paris en 45’\n* Gare TGV Reims Champagne à Bezannes (à 5km du centre de Reims) : 3 A/R pour Paris en 40’\n9 interconnexions avec le réseau national (Roissy Charles De Gaulle – 30’, Marne la Vallée – 30’, Massy – 1h, Strasbourg –1h50, Nantes – 3h15, Rennes – 3h17, Bordeaux – 4h36, Londres – 4h10, Lille – 1h34)\nPour plus de renseignements, connectez-vous sur le site de la SNCF www.voyages-sncf.com\nPar avion :\nAéroport Roissy Charles De Gaulle : à 30’ en TGV\nAéroport Paris-Orly : 1h30\nAéroport Paris-Vatry : 1h en navette (http://www.parisvatry.com/spip.php?article54)\nLiens vers les sites des aéroports :\nwww.aeroportsdeparis.fr\nwww.parisvatry.com\n\n\nSe déplacer à Reims\nEn tramway ou en bus sur le réseau Citura : www.citura.fr\nLes Journées thématiques auront lieu à l’Université de Reims sur le Campus Croix-Rouge, Bâtiment 13 « Recherche », 57 rue Pierre Taittinger. Descendre à l’arrêt « Campus Croix-Rouge » sur les lignes A ou B du tramway.\n\nPlan du Campus Croix-Rouge\nPlan tramway Reims"
  },
  {
    "objectID": "Organisation.html",
    "href": "Organisation.html",
    "title": "Organisation des enseignements",
    "section": "",
    "text": "La formation est pluridisciplinaire. Les enseignements sont dispensés à parts égales par des mathématiciens, des économistes e des professionnels du secteur.\nLa spécialité de Master SEP se compose de deux préiodes.\nAu cours de la première (de début septembre à fin mars), les étudiants suivent un corpus de cours obligatoires :\n\nstatistique/probabilités : séries temporelles, économétrie, analyse des données, Data-Mining, géomarketing, scoring et business intelligence, statistique exploratoire et big data ;\néconomie : prospective, évaluation et calcul économique ;\ninformatique : SAS, SPSS, SPLUS ®, R, VB-VBA, ACCESS-SQL Server, hadoop ;\ntransversal : anglais, séminaire de formation à la recherche, implication dans la vie associative.\n\nSont proposés en option des cours des masters MMSI, EEDD et de l’école de commerce NEOMA. Certains cours ont lieu en anglais. Ces cours se regroupent en :\n\nfinance et gestion des risques : modélisation, mathématiques financières, mesure et gestion des risques, modélisation stochastique en temps continu, techniques statistiques pour l’analyse des risques difficiles à modéliser, introduction au traitement de l’image ;\néconomie : analyse multi-critères, acteurs et politiqueq publiques, gestion du développement durable.\n\nTous les cours sont évalués par des tests et des projets.\nOutre ces cours, les étudiants ont également accès au DataCamp.\nDataCamp est un navigateur pour apprendre les sciences de données dans le confort avec les didacticiels vidéo et les défis de codage sur R, Python, statistiques et d’autres langages de programmation. Le master SEP offre un accès gratuit pendant six mois au DataCamp pour acquérir une expérience pratique des outils et des processus que les scientifiques des données utilisent quotidiennement. Le lien de DataCamp est disponible ici.\nLa seconde période (de début avril à fin septembre) est constituée par le stage.\nLes enseignements du master SEP sont situés sur trois sites : Moulin de La Housse, Croix Rouge et locaux de l’école de commerce NEOMA.\nLa liste des enseignements du master SEP ainsi qu’un descriptif de leur contenu est disponible ici. La liste des intervenants (enseignants-chercheurs en mathématiques ou économie, professionnels des secteurs d’activités ciblés) est disponible ici."
  },
  {
    "objectID": "Enseignements.html",
    "href": "Enseignements.html",
    "title": "Liste des enseignements",
    "section": "",
    "text": "Un programme détaillé des enseignements et des intervenants est disponible en cliquant ici.\n\nProgramme des enseignements du premier semestre\n\n\n\n\n\n\n\n\nintitulés U.E. et E.C.\nE.C.T.S./\nVolume horaire\n\n\n\n\ncoef.\n\n\n\n\nU.E.\nE.C.\nC.M.\n\n\nU.E. 0911 Disciplinaire\n6\n\n\n\nSESG0903 -Evaluation et calcul économiques\n\n1\n\n\nSESG0907 -Evaluation économique avancée\n\n2\n\n\nSESG0902 -Atelier de prospective\n\n1\n\n\nMA0971 –Econométrie\n\n2\n\n\nU.E.0912 Disciplinaire de différenciation\n6\n\n\n\nMA0989 -Fondements des probabilités et statistique et introduction à R\n\n1\n\n\nMA0973 -Géostatistique\n\n1\n\n\nMA0974 -Système d’Information Géographique (SIG)\n\n1\n\n\nSESG0904 -Analyse des données\n\n2\n\n\nMA0976 -Option 1, un E.C. à choisir parmi (*)\n\n1\n\n\nU.E.0913 Disciplinaire de différenciation\n6\n\n\n\nMA0975 -Option 2, un E.C. à choisir parmi (*)\n\n2\n\n\nMA0977 -Initiation à la recherche : Séminaires « recherche » ou projet terrain\n\n2\n\n\nMA0979 -Séries temporelles, Hadoop\n\n2\n\n\nU.E. 0914 Disciplinaire\n6\n\n\n\nMA 0980 -Tests statistiques avancés avec applications sous R\n\n2\n\n\nMA0981 -Data Mining\n\n1\n\n\nMA0982 –Scoring et Business Intelligence\n\n1\n\n\nMA0983 -SAS\n\n1\n\n\nMA0984 -Statistique exploratoire, big data\n\n1\n\n\nU.E. 0915 Compétences Transversales\n6\n\n\n\nMA0985 -Implication dans la vie associative universitaire (IVAU)\n\n1\n\n\nAN0913 -Langue\n\n1\n\n\nMA0986 -ACCESS, SQL Server\n\n1\n\n\nMA0987 -VB-VBA et Excel\n\n1\n\n\nSESG0906 -SPSS\n\n1\n\n\nMA0988 -Techniques de Recherche d’Emploi et de Stage (TRES)\n\n1\n\n\n\n\n\nCours d’approfondissement (options du premier semestre)\n\n\n\n\n\n\n\nCours d’approfondissement : options 1 et 2\nVolume horaire\n\n\n\n\nC.M.\nT.D.\n\n\nMA0941 -Modélisation mathématique (Master MMSI)\n16\n\n\nMA0990 -Mesure et gestion des risques\n20\n\n\nMA0944 -Introduction au traitement d’images (Master MMSI)\n10\n\n\nEEDD0901 -Analyse multicritères (master EEDD)\n15\n\n\nEEDD0902 -Acteurs et politiques publics (Master EEDD)\n20\n\n\nEEDD0903 -Gestion du développement durable (Masters EEDD et Logistique)\n25\n\n\nSESG0901 -Economie du Développement Durable (Master EEDD)\n20\n\n\nRMS0902 -Modélisation stochastique en temps continu (RMS)\n20\n\n\nRMS0901 -Techniques statistiques pour l’analyse des risques difficiles à modéliser (RMS)\n20\n\n\n\n\n\nEnseignements du seconde semestre\n\n\n\n\n\n\n\n\n\n\nintitulés U.E. et E.C.\nE.C.T.S./\nVolume horaire\n\n\n\n\ncoef.\n\n\n\n\nU.E.\nE.C.\nC.M.\n\n\nU.E.1016.1 Disciplinaire de différenciation : Majeure professionnelle\n30\n\n\n\nMA1071 -Méthodologie du mémoire de recherche, mémoire de recherche\n\n8\n\n\nMA1073 -Méthodologie du rapport de stage, stage (de 3 à 6 mois), rapport et soutenance\n\n22\n\n\nU.E.1016.2 Disciplinaire de différenciation : Majeure recherche\n\n\n\n\nMA1072 -Méthodologie du rapport de stage, stage (de 1 à 6 mois), rapport\n\n8\n\n\nMA1074 -Méthodologie du mémoire de recherche, mémoire de recherche, soutenance\n\n22"
  },
  {
    "objectID": "contacts.html",
    "href": "contacts.html",
    "title": "Contacts",
    "section": "",
    "text": "Pour toute demande d’information concernant le master SEP, envoyer un mail à:\n\nResponsable du master SEP, spécialité économie APE :\nEmmanuelle GAUTHERAT\ncourriel : emmanuelle.gautherat@univ-reims.fr\n\n\nResponsable du master SEP, spécialité mathématiques :\nJules MAES\ncourriel : jules.maes@univ-reims.fr\nEmail du Master SEP : contact.mastersepreims@gmail.com\nPage Facebook : https://www.facebook.com/Master-SEP-Reims-108350474430182\nVous souhaitez connaitre l’avis des étudiants sur l’enseignement dispensé et les possibles débouchées ? Posez votre question, les étudiants vous répondrons."
  },
  {
    "objectID": "Enseignants.html",
    "href": "Enseignants.html",
    "title": "Liste des enseignants",
    "section": "",
    "text": "Un programme détaillé des enseignements et des intervenants est disponible en cliquant ici.."
  },
  {
    "objectID": "Stage.html",
    "href": "Stage.html",
    "title": "Stages",
    "section": "",
    "text": "Le master SEP propose un stage long de 4 à 6 mois pour la majeure professionnelle et de 1 à 6 mois pour la majeure recherche.\n\nStages 2017/2018\n\n\n\n\n\n\n\nENTREPRISE ET SUJET\nMISSION\n\n\n\n\nSAS Spring Campus – Développement Avant-vente, Fraude\nEncadré par un responsable senior avec un fort background analytique, vous travaillerez sur l’intégration du Machine Learning au sein de nos solutions de lutte contre la fraude et le blanchiment d’argent.\nLe succès du projet demandera de travailler sur les points suivants :\n– Comprendre la problématique business\n– Rechercher et analyser les approches existantes\n– Proposer de nouveaux modèles\n– Développer & intégrer ces modèles\n– Valider la valeur ajoutée de la nouvelle approche\n– Présenter les résultats\n\n\nRCI banque-DIAC\n– Développement d’un score Marketing pour une des filiales du groupe, comparaison de méthodes de Scoring classiques vs. Machine Learning avec utilisation potentielle de données externes.\n– Mise à jour occasionnelle des monitoring & backtesting\n– Analyses statistiques ponctuelles.\n\n\nBig Apps\nÉtudier et implémenter un système d’interrogation d’un data lake basé sur le langage humain comme système de requête. La restitution la donnée sera soiys forme graohique (machine learning, process NLP).\n\n\nVelvet\nOptimisation des actions marketing : analyses sur le parcours client ; segmentations marketing ; modélisation prédictive ; automatiser et maintenir les tableaux de bord de pilotage d’activité ; études\n\n\nSAS Spring campus – développement avant-vente – Analytics -Traitement de l’image & Deep Learning\nApplication des techniques de deep learning disponibles avec SAS® Visual Data Mining & Machine Learning. SAS® Viya™ apportera ses nouvelles techniques en version 3.3 (Deep forward networks, Auto encoders, Convolutional networks, Recurrent networks).\nIdentifier une problématique métier pouvant bénéficier d’une classification automatique des images.\n– Vous devrez mettre en pratique ces nouvelles techniques de traitement de l’image et les illustrer à partir d’un démonstrateur sur cette problématique métier.\n– Vous pourrez également participer à des présentations internes ainsi qu’à des opportunités clients sur le même sujet.\nCompétences requises :\nMaîtrise du langage SAS, Python, R\nCapacité à s’adapter à de nouveaux langages et technologies\nConnaissances pratiques des fonctions de traitement de l’image\nConnaissances théoriques des réseaux de neurones – deep learning\nConnaissance du concept de temps réel – event stream processing\n\n\nPum plastiques\nTravaux de réflexions et de conception de l’évolution du traif vente (entreprise internationale) : pricing\n\n\nGreenflex\nMise en œuvre opérationnelle du CRM en mode multicanal pour la coopération CRM Services (requêtage sur UNICA – logiciel maison- ciblages ; mise en place projets t tests ; mise en production ; enrichisseemnt ; bilan quanti et quali ; développer des outims\n\n\nCEDEPOD\nEtude des facteurs déterminants des investissements direts étrangers dans les pays de l’Union économique monétaire ouest africain (UEMOA)\n\n\nSAS Spring campus – développement avant-vente – Data management\nEn tant que stagiaire en Data Management, vous participerez à la préparation de propositions pour valider l’adaptabilité des logiciels SAS pour les scénarios IoT et autres « edge computations » (traitement de flux de données en temps réel et Big Data, par exemple) en utilisant les techniques et technologies les plus récentes.\nLe succès du projet demandera de travailler sur les points suivants :\n-Comprendre la problématique business ;\n– Rechercher et analyser les approches existantes ;\n– Proposer de nouvelles approches ;\n– Développer & intégrer l’approche dans le business scenario ;\n– Valider la valeur ajoutée de la nouvelle approche ;\n– Présenter les résultats.\nCompétences requises :\n-Expérience en programmation (certification SAS Base Programming), la connaissance des outils de Data Management serait un plus! ;\n– Vous êtes capable de travailler en équipe et êtes autonome, motivé et avez le sens des responsabilités ;\n– Vous êtes particulièrement à l’aise pour écrire des communications synthétiques de qualité (emails, présentations powerpoint) ; vous vous exprimez couramment en Français et en Anglais ; enfin, vous êtes capable d’animer des présentations devant une audience.\n-Vous pouvez être amenés à voyager ponctuellement en France et en Europe dans le cadre de vos missions.\n\n\nZenpark\n-Création d’applications sur Qlik : consulter les équipes métiers concernées, construire le modèle de données, importer les données, configurer les feuilles de visualisation des données\n– Mise à jour des applications Qlik : suivre les changements en base de données SQL et les répercuter dans les applications Qlik concernées, prendre en compte les changements de process et les prendre en compte dans les applications Qlik concernées\n– Smart-Cities : effectuer l’analyse de l’occupation réelle des parkings, créer un modèle de foisonnement applicable à différents types de parking\n– Etude de rentabilité : créer un outil d’aide à la décision sur la performance des parkings\n– Projection de revenus : créer un outil d’aide à la décision permettant de calculer des projections de revenus selon différents critères\n\n\nCommunauté d’agglo du Saint Quentinois\nAlimentation de Bdd, référent de cette base aec mise en place d’un observatoire + chargé des études stratégiques de la direction. Veille stratégique, lien avec de nombreuse services. Analyser les données note de synthèse, argumenter pour objectif d’aide à la décision.\n\n\nSAS spring campus – développement Avant -vente – Fraude\nÉquipe internationale : présentation, réponses a appels d’offre, créations de démonstrateurs, intégration du machine learning au sein des solutions de lutte contre la fraude et le blanchiment d’argent.\n\n\nSAS Spring campus – développement consulting – Fraude\nMise en œuvre d’une solution SAS de lutte contre la fraude, le blanchiment d’argent et l’abus de marché.\nLe succès du stage demandera de travailler sur les points suivants :\n• Comprendre la problématique business du client\n• Être capable de travailler en équipe\n• Montrer des qualités de proactivité et d’adaptabilité\n• Proposer, développer et intégrer de nouveaux modèles\n• Montrer des qualités rédactionnelles\n\n\nVelvet\nMise en place d’un système de recommandations : contribution à la conception et la mise en place des briques applicatives ; développement de pipelines algorithmiques développement et implémenter des algo de recommandations. Développement en environnement big data (Spark, Hive, Python, ) sur des problématiques d’intégration de données et d’industrialisation du machine learning.\n\n\nOrange SA\nAnalyse de données marketing (vente, achat, stock, prévision de ventes) des pays Orange( zone Europe et Middle-east/Afrique) : def et fiabilisation du process de récupération de donnée(format MS ACCESS), mise en place requetes SQL d’intégration, analyse avec els utilisateurs cibles. Mise en place des requêtes SQL d’analyse (avec VBA+ ThinkCell), propositions d’axes d’analyses marketing.\nAnalyse des sources alternatives de données (réseaux sociaux, usages applicatifs etc).\n\n\nSpring campus\nAVISIA\nIntervention chez un client (star up) : liene nte cienst et fournisseurs de service : dasch board du client, indicateurs etc + matrcie de compétence Avisia, cartographie des activités des collaborateurs\n\n\n\nSpring campus, CA\nanalyse desc des sujets d’études ; enrchissement été xploitation d’un datamart géomarketing ; scores d’appétence ; typologie de clientèle ; rédactiondes rapports d’études ; mise en production industrielle ; R&D ds le domaien big data\n\n\n\nMybestpro (telecommunications)\nAlgo pour système de recommandation, text mining avec prédictionde teux d’acceptation (sur doc de type devis), analyse d’impact.\n\n\n\nSpring campus,\nSG La défense\nRédaction d’un manuel de formation illustrant l’exploration de données en vue de détection de fraude AML potentielle : visual analytics / visual scenario designer ; etude de cas avec des données\nRédaction d’un manuel de formation destiné aux Data scientist illustrant la comparaison des modèles statistiques et des modèles IA : entreprise Miner.\nSAS\n\n\n\nSpring campus,\nSG La défense\nRédaction d’un manuel de formation illustrant l’exploration de données en vue de détection de fraude AML potentielle : visual analytics / visual scenario designer ; etude de cas avec des données\nRédaction d’un manuel de formation destiné aux Data scientist illustrant la comparaison des modèles statistiques et des modèles IA : entreprise Miner.\nSAS\n\n\n\nBNP PARIBAS Securites Services\nmise en œuvre du progiciel AB Initio pour gérer la gourvenance et la qualité des données. : process de A à Z de data management sur un système d’info dédié.\n\n\n\ncaisse d’épargne Bretagne pays de loire\nCréation de modèles statistiques à partir des données sociales pour faire de l’analyse prédictive.Construction d’un portail de reporting RH pour l’ensemble de la CEBPL\nSQL et R\n\n\n\nBRGM\nInitier une base de données d’apprentissage robuste, réunissant des enregistrements sismiques (larges bandes et/ou accéléromètriques), accompagnés de nombreuses métadonnées, issus de différentes bases de données internationales (e.g. K-NET and KiK-net, PEER, ESM, etc.). Réaliser le code qui permet de transformer ces ensembles de données en un problème abordable par des méthodes de data science (feature engineering), notamment dans un cadre d’apprentissage automatique supervisé. Tester plusieurs approches d’apprentissage artificiel (e.g “random forest”, “xgboost”, “SVM”, “deep learning”…) sur certaines statistiques du signal sismique, établis sur une fenêtre de temps glissante, comme caractéristiques descriptives. La sélection des données d’entrée, des caractéristiques descriptives, de la taille de la fenêtre de temps et des méta- paramètres des algorithmes de machine learning sont autant de variables du problème à déterminer. Réaliser le code permettant la comparaison minutieuse des résultats à travers une campagne de tests systématiques. Les données étant potentiellement très volumineuses, le test intensif et systématique de nombreux modèles engendrera probablement un problème de temps de calcul. Le cas échéant, nous projetons de prendre en compte cette difficulté en intégrant et adaptant les algorithmes d’apprentissage à un cadre de calculs distribués, comme celui proposé par l’environnement Big Data du BRGM\n\n\n\nPum plastiques\nÉtudier et analyser le fichier clients grands comptes : mettre à jour le fichier avec els SIRTE ; créer et mettre en place un process d’enregistrement mensuel des données clients ; élaborer et mettre en forme les stat clients ; proposer des axes d’amélioration de l’outil.\n\n\n\nPrisma média\nPôle data science : Stock sur Hadoop /Spark, requêtage avec Dataiku, R, Python. Réaliser des scores, enrichissement de la BDD via open data, prédiction d’audience (modele de prediction), segments( avec google analytics), analsye pression commerciale (email). Le stagiaire participera également aux missions big data : branchements Hadoop/Spark/R avec l’équipe IT ; etc (voir fiche mission)\n\n\n\nUmanis SA\nIntégration d’une équipe de consultants SAS pour internvenir sur la maintenance applicative des applications SAS d’und es clients grand compte : 80% pges SAS sous UNIX, 20% pg shell ; tests unitaires et d’intégration ; analyse des demandes d’évolutions ; livraison enrecette."
  },
  {
    "objectID": "parle_de_nous.html",
    "href": "parle_de_nous.html",
    "title": "On parle de nous !",
    "section": "",
    "text": "Article du magazine Challenge\nArticle du journal Le Monde\nClassement SMBG 2018 – meilleurs Masters Big Data\nSAS Joint Certificate Program"
  },
  {
    "objectID": "Débouchés.html",
    "href": "Débouchés.html",
    "title": "Débouchés",
    "section": "",
    "text": "Métiers\nLes métiers visés sont très variés : chargé d’études statistiques ou économiques ; data-miner ; gestionnaire de risques ; géomarketeur ; économètre ; analyste financier ; marketeur quantitatif.\nLa rémunération moyenne annuelle brute constatée pour le premier emploi est 32 K€ et le délai d’attente pour ce premier emploi est généralement inférieur à 6 mois.\nL’annuaire des anciens étudiants diplômés de SEP présente les postes occupés tout au long de leur carrière (à demander à l’association du master).\n\n\nSecteurs d’activités\nLe master S.E.P. permet de s’insérer facilement dans des secteurs d’activités aussi divers que ceux :\n– des cabinets de conseil statistique et économique : prospective, marketing quantitatif, gestion des risques, géomarketing, audimat, etc…\n– des administrations : locales, nationales, internationales;\n– de l’industrie et des services : banques, assurances, pharmacie, etc…"
  },
  {
    "objectID": "Formation.html",
    "href": "Formation.html",
    "title": "Formation",
    "section": "",
    "text": "Le Master Statistique pour L’évaluation offre aux étudiants l’opportunité d’acquérir une formation sur deux ans dans les domaines de l’analyse des données à large échelle et de l’aide à la décision.\nLa master 2 SEP regroupe les étudiants issus du master 1 mathématiques et Applications de la faculté des sciences et technologies et le master 1 Analyse et politique économique de la faculté de lettres droit économie et gestion. Le master accueille aussi des étudiants d’autres universités françaises et étrangères.\nCette mutualité permet aux étudiants d’échanger entre eux leurs compétences.\nOrganisation des enseignements du master SEP Mathématiques et applications.\nListe des enseignements du master SEP.\nListe des enseignants du master SEP.\nStage."
  },
  {
    "objectID": "service_logement.html",
    "href": "service_logement.html",
    "title": "Service logement",
    "section": "",
    "text": "Le CROUS est la principale structure pour obtenir un logement en résidence universitaire. Mais la chatoyante ville de Reims à beaucoup de logements chez des particuliers comme chez des privés, qui ne demande qu’à accueillir des étudiants.\nPour les étudiants étrangers, les démarches sont d’autant plus difficiles du point de vue administratif. Le livret d’accueil ci-dessous, spécifiquement dédié aux étudiants étrangers, résume l’ensemble des informations nécessaires à leur intégration.\nLivret d’accueil 2020-2021"
  },
  {
    "objectID": "vie_etudiante.html",
    "href": "vie_etudiante.html",
    "title": "vie_etudiante",
    "section": "",
    "text": "Plans d’accès\nService logement"
  },
  {
    "objectID": "ressources.html",
    "href": "ressources.html",
    "title": "Ressources",
    "section": "",
    "text": "titre <- \"Carriere de la joueuse\"\n\nvalueBox(value = titre, subtitle = \"Page 1\", color = \"lime\")\n\n\n\n\n\nPage 1"
  },
  {
    "objectID": "ressources.html#row-1",
    "href": "ressources.html#row-1",
    "title": "Ressources",
    "section": "row",
    "text": "row\n\nClassement WTA\nWomen’s Tennis Association\nWTA est la principale association organisant les compétitions féminines de tennis. Aujourd’hui, l’importance des tournois féminins est égale à celle des tournois masculins ce qui n’était pas le cas à la création de la WTA en 1973. De plus, elle classe, tout au long de la saison, l’ensemble des joueuses actives en fonction de leurs performances.\nIci, vous aurez un aperçu de la carrière de , en commençant par sa carrière en générale depuis les années 2000, puis un focus sur la saison de votre choix et enfin un focus sur le tournoi de votre choix."
  },
  {
    "objectID": "Actualites.html",
    "href": "Actualites.html",
    "title": "Actualités",
    "section": "",
    "text": "10 bases de données orientées machine learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDégradation de la qualité et l’accessibilité des données en 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft lance officiellement SQL Server 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSalaires 2023 : +19% d’augmentation pour les data scientists\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Actualites.html#running-code",
    "href": "Actualites.html#running-code",
    "title": "Actualités",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "Articles/Article_1.html",
    "href": "Articles/Article_1.html",
    "title": "10 bases de données orientées machine learning",
    "section": "",
    "text": "Bien que leurs approches et leurs capacités diffèrent, de nombreuses bases de données permettent de créer des modèles d’apprentissage automatique pour exploiter et donner du sens à des mines d’informations. Aux côtés des mastodontes Amazon, Google, Oracle et Microsoft des plateformes comme BlazingSQL, Brytlyt et Kinetica sortent du lot.\nDans un article d’octobre 2022 intitulé Comment choisir une plateforme d’apprentissage automatique cloud, le premier conseil de Martin Heller pour le choix d’une plateforme était le suivant : « Soyez proche de vos données ». Garder le code près des données est nécessaire pour maintenir une faible latence, car la vitesse de la lumière limite les vitesses de transmission. Après tout, le machine learning (ML) - en particulier le deep learning - a tendance à parcourir toutes vos données plusieurs fois. L’idéal, pour les très grands ensembles de données, est de construire le modèle là où les données résident déjà, de sorte qu’aucune transmission massive de données ne soit nécessaire. Plusieurs bases de données le permettent dans une certaine mesure. La question suivante est naturellement : quelles bases de données prennent en charge le ML interne, et comment le font-elles ? Voici un tour d’horizon de ces bases de données par ordre alphabétique."
  },
  {
    "objectID": "Articles/Article_1.html#amazon-redshift",
    "href": "Articles/Article_1.html#amazon-redshift",
    "title": "10 bases de données orientées machine learning",
    "section": "Amazon Redshift",
    "text": "Amazon Redshift\nAmazon Redshift est un service managé d’entrepôt de données à l’échelle du pétaoctet, conçu pour rendre simple et rentable l’analyse de toutes les données à l’aide d’outils de veille stratégique existants. Il est optimisé pour des ensembles de données allant de quelques centaines de gigaoctets à un pétaoctet ou plus et coûte moins de 1 000 $ par téraoctet et par an. Redshift ML est conçu pour permettre aux utilisateurs de SQL de créer, former et déployer facilement des modèles de ML à l’aide de commandes SQL. CREATE MODEL de Redshift SQL définit par exemple les données à utiliser pour la formation et la colonne cible, puis les transmet à SageMaker Autopilot pour la formation via un bucket Amazon S3 chiffré dans la même zone.\n\nFonctionnement de Redshift ML. (Crédit : AWS)\nAprès l’entraînement AutoML, Redshift ML compile le meilleur modèle et l’enregistre en tant que fonction SQL de prédiction dans votre cluster Redshift. Vous pouvez ensuite invoquer le modèle pour l’inférence en appelant la fonction de prédiction dans une instruction SELECT. En résumé, Redshift ML utilise SageMaker Autopilot pour créer automatiquement des modèles de prédiction à partir des données que vous spécifiez via une instruction SQL, qui sont extraites dans un bucket S3. La meilleure fonction de prédiction trouvée est enregistrée dans le cluster Redshift."
  },
  {
    "objectID": "Articles/Article_1.html#blazingsql",
    "href": "Articles/Article_1.html#blazingsql",
    "title": "10 bases de données orientées machine learning",
    "section": "BlazingSQL",
    "text": "BlazingSQL\nBlazingSQL est un moteur SQL accéléré par le GPU construit sur l’écosystème RAPIDS ; il existe sous forme de projet open source et de service payant. RAPIDS est une suite de bibliothèques logicielles et d’API open source, incubée par Nvidia, qui utilise CUDA et est basée sur le format de mémoire en colonnes Apache Arrow. CuDF, qui fait partie de RAPIDS, est une bibliothèque GPU DataFrame de type Pandas permettant de charger, de joindre, d’agréger, de filtrer et de manipuler des données. Dask, également open source, apporte de la mise à l’échelle des paquets Python sur plusieurs machines. Dask peut distribuer les données et les calculs sur plusieurs GPU, soit dans le même système, soit dans un cluster multi-nœuds. Dask s’intègre à RAPIDS cuDF, XGBoost et RAPIDS cuML pour l’analyse de données et l’apprentissage automatique accélérés par le GPU.\nAinsi, BlazingSQL peut exécuter des requêtes accélérées par le GPU sur des data lake dans Amazon S3, transmettre les DataFrames résultants à cuDF pour la manipulation des données, et enfin effectuer l’apprentissage automatique avec RAPIDS XGBoost et cuML, et l’apprentissage profond avec PyTorch et TensorFlow."
  },
  {
    "objectID": "Articles/Article_1.html#brytlyt",
    "href": "Articles/Article_1.html#brytlyt",
    "title": "10 bases de données orientées machine learning",
    "section": "Brytlyt",
    "text": "Brytlyt\nBrytlyt est une plateforme basée sur un navigateur qui permet de traiter de l’IA dans une base de données avec des capacités de deep learning. Brytlyt combine une base de données PostgreSQL, PyTorch, Jupyter Notebooks, Scikit-learn, NumPy, Pandas et MLflow en une seule plateforme serverless qui sert de trois produits accélérés par GPU : une base de données, un outil de visualisation de données et un outil de science des données qui utilise des notebooks. Brytlyt se connecte à tout produit disposant d’un connecteur PostgreSQL, y compris les outils de BI tels que Tableau, et Python. Il supporte le chargement et l’ingestion de données à partir de fichiers de données externes tels que les CSV et à partir de sources de données SQL externes supportées par les wrappers de données étrangères (FDW) de PostgreSQL. Parmi ces dernières, citons Snowflake, Microsoft SQL Server, Google Cloud BigQuery, Databricks, Amazon Redshift et Amazon Athena.\nEn tant que base de données GPU avec traitement parallèle des jointures, Brytlyt peut traiter des milliards de lignes de données en quelques secondes. Brytlyt a des applications dans les télécommunications, le commerce de détail, le pétrole et le gaz, la finance, la logistique, l’ADN et la génomique. Avec PyTorch et Scikit-learn intégrés, Brytlyt peut supporter à la fois le deep learning et les modèles simples de ML fonctionnant en interne contre ses données. La prise en charge des GPU et le traitement parallèle signifient que toutes les opérations sont relativement rapides, même si l’entraînement de modèles d’apprentissage profond complexes sur des milliards de lignes prendra bien sûr un certain temps."
  },
  {
    "objectID": "Articles/Article_1.html#google-cloud-bigquery",
    "href": "Articles/Article_1.html#google-cloud-bigquery",
    "title": "10 bases de données orientées machine learning",
    "section": "Google Cloud BigQuery",
    "text": "Google Cloud BigQuery\nBigQuery est le datawarehouse infoféré de Google Cloud, à l’échelle du pétaoctet, pour exécuter des analyses sur de grandes quantités de données en temps quasi réel. BigQuery ML est utilisé pour créer et exécuter des modèles de ML dans BigQuery à l’aide de requêtes SQL. Ce data warehouse prend en charge la régression linéaire pour les prévisions, la régression logistique binaire et multi-classes pour la classification, le clustering K-means pour la segmentation des données, la factorisation matricielle pour la création de systèmes de recommandation de produits, les séries temporelles pour les prévisions de séries temporelles, y compris les anomalies, la saisonnalité et les jours fériés, les modèles de classification et de régression XGBoost, les réseaux neuronaux profonds basés sur TensorFlow pour les modèles de classification et de régression, les tableaux AutoML et l’importation de modèles TensorFlow.\n\nGoogle propose de faciliter les analyses en rassemblant les données provenant de plusieurs sources dans BigQuery. (Crédit : Google)\nVous pouvez utiliser un modèle avec des données provenant de plusieurs ensembles de données BigQuery pour l’entraînement et pour la prédiction. BigQuery ML n’extrait pas les données de l’entrepôt de données. Vous pouvez effectuer une ingénierie des fonctionnalités avec BigQuery ML en utilisant la clause TRANSFORM dans votre instruction CREATE MODEL. Selon Martin Heller, BigQuery ML apporte une grande partie de la puissance de Google Cloud Machine Learning dans l’entrepôt de données BigQuery avec la syntaxe SQL, sans extraire les données de l’entrepôt de données."
  },
  {
    "objectID": "Articles/Article_1.html#ibm-db2-warehouse",
    "href": "Articles/Article_1.html#ibm-db2-warehouse",
    "title": "10 bases de données orientées machine learning",
    "section": "IBM Db2 Warehouse",
    "text": "IBM Db2 Warehouse\nIBM Db2 Warehouse on Cloud est un service de cloud public managé. Il est possible de configurer cet environnement sur site avec son propre matériel ou dans un cloud privé. En tant qu’entrepôt de données, il comprend des fonctionnalités telles que le traitement des données en mémoire et les tableaux en colonnes pour le traitement analytique en ligne. Sa technologie Netezza fournit un ensemble robuste d’analyses conçues pour amener efficacement la requête aux données. Une gamme de bibliothèques et de fonctions vous aide à obtenir les informations précises dont on peut avoir besoin.\n\nObtenez un aperçu rapide de votre historique d’utilisation du stockage et de l’activité de chargement à l’aide du tableau de bord de la console Web. (Crédit : IBM)\nDb2 Warehouse prend en charge le ML dans la base de données en Python, R et SQL. Le module IDAX contient des procédures analytiques stockées, notamment l’analyse de la variance, les règles d’association, la transformation des données, les arbres de décision, les mesures de diagnostic, la discrétisation et les moments, le regroupement K-means, les voisins les plus proches, la régression linéaire, la gestion des métadonnées, la classification naïve Bayes, l’analyse en composantes principales, les distributions de probabilités, l’échantillonnage aléatoire, les arbres de régression, les modèles et règles séquentiels, ainsi que les statistiques paramétriques et non paramétriques."
  },
  {
    "objectID": "Articles/Article_1.html#kinetica",
    "href": "Articles/Article_1.html#kinetica",
    "title": "10 bases de données orientées machine learning",
    "section": "Kinetica",
    "text": "Kinetica\nKinetica Streaming Data Warehouse combine l’analyse de données historiques et en continu avec l’intelligence de localisation et l’IA dans une seule plateforme, le tout accessible via API et SQL. Kinetica est une base de données très rapide, distribuée, en colonnes, privilégiant la mémoire et accélérée par les GPU, avec des fonctionnalités de filtrage, de visualisation et d’agrégation.\nKinetica intègre des modèles et des algorithmes d’apprentissage automatique avec vos données pour une analyse prédictive en temps réel à l’échelle. Il vous permet de rationaliser vos pipelines de données et le cycle de vie de vos analyses, modèles d’apprentissage automatique et ingénierie des données, et de calculer des fonctionnalités avec le streaming. Kinetica fournit une solution de cycle de vie complet pour l’apprentissage automatique accéléré par les GPU : des carnets Jupyter gérés, l’entraînement des modèles via RAPIDS, et le déploiement et l’inférence automatisés des modèles dans la plateforme Kinetica. Il s’agit d’une solution complète de cycle de vie dans la base de données pour le ML accéléré par les GPU, et peut calculer des caractéristiques à partir de données en continu."
  },
  {
    "objectID": "Articles/Article_1.html#microsoft-sql-server",
    "href": "Articles/Article_1.html#microsoft-sql-server",
    "title": "10 bases de données orientées machine learning",
    "section": "Microsoft SQL Server",
    "text": "Microsoft SQL Server\nLes services d’apprentissage automatique de Microsoft SQL Server prennent en charge R, Python, Java, la commande T-SQL PREDICT et la procédure stockée rx_Predict dans le SGBDR SQL Server, et SparkML dans les grappes de données Big Data de SQL Server. Dans les langages R et Python, Microsoft inclut plusieurs paquets et bibliothèques pour le ML. Vous pouvez stocker vos modèles formés dans la base de données ou en externe. Azure SQL Managed Instance prend en charge Machine Learning Services for Python et R en preview.\n\nPour lancer l’installation, il suffit de démarrer l’assistant de configuration de SQL Server, puis dans l’onglet Installation, sélectionner Nouvelle installation autonome de SQL Server ou ajouter des fonctionnalités à une installation existante. (Crédit : Microsoft)\nMicrosoft R dispose d’extensions qui lui permettent de traiter les données à partir du disque ainsi qu’en mémoire. SQL Server fournit un cadre d’extension pour que le code R, Python et Java puisse utiliser les données et les fonctions de SQL Server. Les clusters Big Data de SQL Server exécutent SQL Server, Spark et HDFS dans Kubernetes. Lorsque SQL Server appelle le code Python, il peut à son tour invoquer Azure Machine Learning, et enregistrer le modèle résultant dans la base de données pour l’utiliser dans les prédictions."
  },
  {
    "objectID": "Articles/Article_1.html#mindsdb",
    "href": "Articles/Article_1.html#mindsdb",
    "title": "10 bases de données orientées machine learning",
    "section": "MindsDB",
    "text": "MindsDB\nSi votre base de données ne prend pas encore en charge l’apprentissage automatique interne, il est probable que vous puissiez ajouter cette fonctionnalité en utilisant MindsDB, qui s’intègre à une demi-douzaine de bases de données et à cinq outils de BI. Les bases de données prises en charge sont MariaDB, MySQL, PostgreSQL, ClickHouse, Microsoft SQL Server et Snowflake. Une intégration avec MongoDB est en cours et des intégrations avec des bases de données en continu sont promises pour 2021. Les outils de BI pris en charge sont actuellement SAS, Qlik Sense, Microsoft Power BI, Looker et Domo.\nMindsDB propose AutoML, les tableaux d’IA et l’IA explicable (XAI). Vous pouvez invoquer la formation AutoML à partir de MindsDB Studio, d’une instruction SQL INSERT ou d’un appel API Python. La formation peut éventuellement utiliser les GPU et créer un modèle de série chronologique. Vous pouvez enregistrer le modèle sous forme de table de base de données et l’appeler à partir d’une instruction SQL SELECT sur le modèle enregistré, à partir de MindsDB Studio ou d’un appel API Python. Vous pouvez évaluer, expliquer et visualiser la qualité du modèle à partir de MindsDB Studio. Il est également possible de connecter MindsDB Studio et l’API Python à des sources de données locales et distantes. MindsDB fournit en outre un cadre d’apprentissage profond simplifié, Lightwood, qui fonctionne sur PyTorch. En clair, MindsDB apporte des fonctionnalités utiles de ML à un certain nombre de bases de données qui ne disposent pas d’une prise en charge intégrée de cette technologie.\n\nA l’aide de données historiques, la solution MindsDB aide à prédire l’avenir. (Crédit : MindsDB)\nUn nombre croissant de bases de données prennent en charge l’apprentissage automatique en interne. Le mécanisme exact varie, et certains sont plus performants que d’autres. Si vous avez tellement de données que vous devriez autrement ajuster des modèles sur un sous-ensemble échantillonné, alors l’une des huit bases de données énumérées ci-dessus - et d’autres avec l’aide de MindsDB - pourrait aider à construire des modèles à partir de l’ensemble des données sans générer de frais généraux importants pour l’exportation des données."
  },
  {
    "objectID": "Articles/Article_1.html#oracle-database",
    "href": "Articles/Article_1.html#oracle-database",
    "title": "10 bases de données orientées machine learning",
    "section": "Oracle Database",
    "text": "Oracle Database\nOracle Cloud Infrastructure (OCI) Data Science est une plateforme managée et serverless destinée aux équipes de science des données pour construire, former et gérer des modèles de ML à l’aide d’Oracle Cloud Infrastructure, notamment Autonomous Database et Autonomous Data Warehouse. Elle comprend des outils, des bibliothèques et des packages centrés sur Python développés par la communauté open source et la bibliothèque Oracle Accelerated Data Science (ADS), qui prend en charge le cycle de vie de bout en bout des modèles prédictifs : acquisition, profilage, préparation et visualisation des données ; ingénierie des caractéristiques ; formation au modèle (y compris Oracle AutoML) ; évaluation, explication et interprétation du modèle (y compris Oracle MLX) ; déploiement de modèles vers Oracle Functions.\nOCI Data Science s’intègre au reste de la pile Oracle Cloud Infrastructure, notamment aux fonctions, au flux de données, à l’entrepôt de données autonome et au stockage d’objets. Les modèles actuellement pris en charge incluent : Oracle AutoML, Keras, Scikit-learn, XGBoost, ADSTuner (réglage des hyperparamètres). A noter qu’ADS prend également en charge l’explicabilité de l’apprentissage automatique (MLX)."
  },
  {
    "objectID": "Articles/Article_1.html#vertica",
    "href": "Articles/Article_1.html#vertica",
    "title": "10 bases de données orientées machine learning",
    "section": "Vertica",
    "text": "Vertica\nVertica Analytics Platform est un entrepôt de données évolutif à stockage en colonnes. Il fonctionne en deux modes : Enterprise, qui stocke les données localement dans le système de fichiers des nœuds qui constituent la base de données, et EON, qui stocke les données de manière communautaire pour tous les nœuds de calcul.\nVertica utilise un traitement massivement parallèle pour traiter des pétaoctets de données, et effectue son apprentissage automatique interne avec le parallélisme des données. Il dispose de huit algorithmes intégrés pour la préparation des données, de trois algorithmes de régression, de quatre algorithmes de classification, de deux algorithmes de clustering, de plusieurs fonctions de gestion des modèles et de la possibilité d’importer des modèles TensorFlow et PMML formés ailleurs. Une fois que vous avez ajusté ou importé un modèle, vous pouvez l’utiliser pour la prédiction. Vertica permet également des extensions définies par l’utilisateur et programmées en C++, Java, Python ou R ainsi que la syntaxe SQL pour la formation et l’inférence.\nArticle rédigé par Anirban Ghoshal sur “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Articles/Article_2.html",
    "href": "Articles/Article_2.html",
    "title": "Microsoft lance officiellement SQL Server 2022",
    "section": "",
    "text": "Microsoft a rendu SQL Server 2022 en disponibilité générale. Plusieurs évolutions sont à noter, notamment une forte imbrication avec le cloud Azure.\nLa déclinaison 2022 du SGBD de Microsoft, SQL Server, passe en mode GA (disponibilité générale). La firme de Redmond l’a annoncé à l’occasion du PASS (Professional Association for SQL Server) Community Summit qui s’est tenu récemment à Seattle. Il succède à SQL Server 2019, sorti il y a un peu plus de trois ans.\nPlusieurs évolutions sont à noter dans la base de données avec une prédominance autour du cloud Azure. Ainsi, dans le cadre d’un PRA, SQL server basculera vers Azure SQL Managed Instance. Par ailleurs, il existe une intégration spécifique avec Azure Synapse - un service datawarehouse et d’analyse de données qui comprend Apache Spark - et Azure Purview, pour la classification et la protection des données. Toujours sur ce dernier point, SQL Server 2022 supporte l’API AWS S3, également prise en charge par d’autres fournisseurs de stockage. Les utilisateurs peuvent ainsi élaborer des scénarios de sauvegarde et de restauration vers S3.\nUne autre fonctionnalité liée au cloud est un modèle optionnel de facturation basé sur Azure Arc (plateforme de cloud hybride), qui fait désormais partie du processus de configuration de SQL Server 2022. Azure Arc est capable de gérer SQL Server depuis Azure, ainsi que d’utiliser des services Azure tels que l’analyse des logs et Azure defender. Les utilisateurs peuvent payer à l’heure, en augmentant la consommation lors des pics de charge et en la diminuant pendant les périodes creuses."
  },
  {
    "objectID": "Articles/Article_2.html#accélérer-les-requêtes-et-améliorer-le-langage-t-sql",
    "href": "Articles/Article_2.html#accélérer-les-requêtes-et-améliorer-le-langage-t-sql",
    "title": "Microsoft lance officiellement SQL Server 2022",
    "section": "Accélérer les requêtes et améliorer le langage T-SQL",
    "text": "Accélérer les requêtes et améliorer le langage T-SQL\nLes performances ont été améliorées comme par exemple T-SQL, le langage de requête de SQL Server. Il comprend des fonctions supplémentaires autour de JSON (JavaScript Object Notation), des manipulations de bits comme LEFT_SHIFT et GET_BIT ou des séries chronologiques. Par ailleurs, il intègre une nouvelle expression IS DISTINCT FROM qui simplifie le traitement des valeurs nulles dans les expressions booléennes.\nL’optimisation des requêtes est également au rendez-vous avec Query Store. La fonction, qui capture l’historique des requêtes et ajuste les performances, est désormais activée par défaut. Elle était auparavant désactivée en raison d’un léger impact sur les performances. A travers ces différentes évolutions et améliorations, Microsoft espère consolider sa base installée face à une concurrence de plus en plus forte dans le domaine des bases de données notamment par les acteurs cloud comme AWS ou Google Cloud.\nArticle rédigé par Jacques Cheminat, rédacteur en chef, sur “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Articles/Article_3.html",
    "href": "Articles/Article_3.html",
    "title": "Les technologies en IA et analyse de données loin de leur plein potentiel",
    "section": "",
    "text": "En France, 63% des managers utilisent des technologies d’intelligence artificielle et d’analyse des données pour assurer leurs missions, selon une étude réalisée par Axys Consultants. Mais plusieurs facteurs, comme le manque de formation ou d’acculturation à ces outils les empêchent d’en tirer pleinement parti.\nDans l’Hexagone, les technologies d’IA et d’analyse des données sont les plus utilisées dans le cadre de la fonction managériale mais des freins au développement de ces dernières sont ressentis plus vivement. C’est le principal enseignement d’une étude nationale réalisée par Axys Consultants auprès de 220 cadres dirigeants. Les résultats montrent en effet que près de 2/3 des managers s’appuient sur ce type de solutions pour assurer leurs missions (63 %). Les outils big data (53 %) sont les plus utilisés par les cadres dirigeants, devant l’IA qui se place en 2ème position (43 %). Selon le cabinet, ces scores laissent supposer que les projets prévus en 2021 ont été menés à bien.  En effet, les répondants étaient 35 % à travailler sur le sujet et 9 % à avoir mis en œuvre des solutions concrètes faisant appel à l’IA (soit 44 %).\nLoin derrière on trouve la réalité virtuelle (12%), la réalité augmentée (6%) et le metaverse qui fait une timide percée à 2 %. A noter tout de même que plus d’un tiers des managers ne fait appel à aucune application citée dans ce classement. Le recours aux moteurs de recherche sémantique est en baisse par rapport à l’an dernier (25 % par rapport à 35 % en 2021), tout comme celui aux chatbots/voicebots (22 % contre 40 %). A l’inverse, les logiciels de reconnaissance à partir du texte ou de la voix, bien qu’encore peu usitées, sont 2 fois plus employées par les managers en 2022 (10 % contre 5 % en 2021)."
  },
  {
    "objectID": "Articles/Article_3.html#miser-sur-la-formation-en-réduisant-les-coûts",
    "href": "Articles/Article_3.html#miser-sur-la-formation-en-réduisant-les-coûts",
    "title": "Les technologies en IA et analyse de données loin de leur plein potentiel",
    "section": "Miser sur la formation en réduisant les coûts ",
    "text": "Miser sur la formation en réduisant les coûts \nMalgré l’intérêt croissant des directions générales pour ces solutions d’automatisation des tâches, certains freins sont évoqués de manière plus exacerbée quant à leur utilisation. Le premier handicap reste l’insuffisance de la formation et l’acculturation des utilisateurs à ces outils, un constat qui a plus que doublé en passant à 73 %en 2022 (contre 33 % l’an dernier). Pour preuve, plus de la moitié des décideurs (57 %) interrogés par Axys souhaitent renforcer leurs compétences dans les domaines de l’IA,  du machine et du deep learning et des data sciences. Ils sont également demandeurs de formation à la stratégie digitale (43 %). L’apprentissage de l’usage des outils collaboratifs dans le management est important pour plus d’un quart des répondants (27 %), tandis que l’acquisition de savoir-faire dans les méthodes agiles et le lean management et demandée par 20 % d’entre eux.\nAprès la montée en compétences, le deuxième point noir au développement de l’IA et des data dans les entreprises est le coût (cité par 55 % des répondants contre seulement 11 % en 2021). Parmi les autres freins, on trouve également la crainte de ne pas pouvoir expliquer les résultats de solutions comme l’IA, citée par 37% de sondés.  Les questions d’éthique qui étaient 3eme en 2021, rétrogradent à la 4ème place mais avec un pourcentage plus élevé (35 % contre 21 %). Si tous ces bémols ont augmenté en intensité, le seul qui échappe à cette règle est la crainte d’être remplacée par un robot qui est 2 fois moins prégnante (10 % contre 20 % en 2021)."
  },
  {
    "objectID": "Articles/Article_3.html#des-attentes-toujours-fortes-sur-lautomatisation",
    "href": "Articles/Article_3.html#des-attentes-toujours-fortes-sur-lautomatisation",
    "title": "Les technologies en IA et analyse de données loin de leur plein potentiel",
    "section": "Des attentes toujours fortes sur l’automatisation",
    "text": "Des attentes toujours fortes sur l’automatisation\nLe classement des attentes des cadres dirigeants vis-à-vis des technologies a également été modifié en 2022. Seule l’automatisation des tâches conserve sa 1ère place tout en affichant une très forte progression (67 % vs 44 %). Vient ensuite la possibilité d’améliorer l’action collective, citée par plus de la moitié des répondants (53 %). L’accroissement des performances remonte sur la troisième marche du podium avec un score qui a plus que quadruplé par rapport à 2021 à 51 %. Les items suivants affichent également de très fortes progressions par rapport à 2021. L’objectif d’utiliser la technologie / IA /big data pour aider le manager à prédire son activité et celle de ses équipes fait plus que tripler (41 % contre 16 %) et triple quasiment pour celui de mieux anticiper les risques (35 % contre 13 %).   \nArticle rédigé par Véronique Arène, journaliste, sur le “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Articles/Article_5.html",
    "href": "Articles/Article_5.html",
    "title": "Salaires 2023 : +19% d’augmentation pour les data scientists",
    "section": "",
    "text": "Comme chaque année, le cabinet de recrutement Robert Walters a publié son étude de rémunération pour 2023. Celle-ci prévoit une moyenne des augmentations salariales de 7% dans les métiers des data, des systèmes d’information et du marketing digital. En tête des plus fortes progressions, on trouve les data scientists de niveau expérimentés, suivis par les directeurs de projets IT.\nEn 2023, la rémunération restera le premier critère de satisfaction pour les cadres en France, notamment dans ce contexte inflationniste qui poussera 71% d'entre eux à demander une augmentation. Dans le secteur numérique, les salaires en 2022 étant élevées, cette tendance devrait s'accentuer en 2023. C'est ce qui ressort de la 24eme étude de rémunérations 2023 présentée par le cabinet Robert Walters ce mardi 13 décembre. Dans l'IT, des hausses de salaires sont attendues « notamment sur des profils d'analystes des données où les entreprises sont prêtes à s'aligner sur les demandes des meilleurs candidats », prévoit ce rapport.\nAinsi, il est prévu une moyenne des augmentations de7% en 2023 dans la catégorie des systèmes d'information et des data. En tête du palmarès, on trouve le data scientist avec 19% de hausse attendue l'an prochain sur le bulletin de salaire d'un profil ayant entre 5 et 10 ans d'expérience. Avec +14% de hausse sur leur fiche de paye, les directeurs de projet confirmés (avec 15 ans et plus d'expérience) ainsi que les « lead » data scientists ayant jusqu'à 10 ans d'expertise feront partie des mieux lotis, selon les projections du cabinet. Les raisons de cette surenchère salariale ? Une très forte demande des entreprises dans ces spécialités. Ainsi, en 2022, les offres d'emploi ont bondi de 77% dans le domaine des data par rapport à 2021 et de 62% pour les project managers."
  },
  {
    "objectID": "Articles/Article_5.html#des-offres-demploi-it-en-constante-progression",
    "href": "Articles/Article_5.html#des-offres-demploi-it-en-constante-progression",
    "title": "Salaires 2023 : +19% d’augmentation pour les data scientists",
    "section": "Des offres d’emploi IT en constante progression ",
    "text": "Des offres d’emploi IT en constante progression \n\nLa pénurie de profils IT a pour effet de faire progresser la moyenne des augmentations de salaires d’ici l’an prochain. (Source RobertWalters/Crédit image Robert Walters)\nA leurs côtés, les professionnels du marketing digital et des ventes dopés notamment par l'importance des stratégies commerciales numériques et le boom des achats en ligne devaient être valorisés de 7% en moyenne. Dans ces secteurs d'activité, deux postes se détachent en termes de progression salariale. Il s'agit du directeur e-commerce, soit +13% revalorisation pour un profil possédant 6 à 12 ans d'expérience et du directeur du digital (+9%) ayant exercé pendant 10 à 15 ans.\n\nDynamiques, les professions en marketing digital et e-commerce devraient être revalorisées en 2023.(Source RobertWalters/Crédit image Robert Walters)"
  },
  {
    "objectID": "Articles/Article_5.html#un-marché-porteur-malgré-le-contexte-économique",
    "href": "Articles/Article_5.html#un-marché-porteur-malgré-le-contexte-économique",
    "title": "Salaires 2023 : +19% d’augmentation pour les data scientists",
    "section": "Un marché porteur malgré le contexte économique",
    "text": "Un marché porteur malgré le contexte économique\nLes entreprises cherchent à constituer des équipes métiers et techniques où les expertises (data science, marketing digital, UX/UI, e- et m-commerce) complètent les fonctions traditionnelles (ERP, infrastructure, sécurité), fait remarquer cette étude. De même, les profils hybrides combinant technicité et approche business, capables de monter en puissance et de devancer une feuille de route IT & digitale en perpétuelle évolution seront également les plus prisés. En outre, au vu de l'explosion des cyberattaques les postes en cybersécurité deviennent une priorité pour les entreprises. Le lancement de projets de transformation vers le cloud, tout en intégrant la composante data, est aussi un enjeu fort à prendre en compte pour les entreprises.\nLe marché IT est totalement épargné par la situation économique pourtant préoccupante, souligne le cabinet de recrutement. La demande n'a jamais été aussi forte. Les candidats sont en position de force sur le marché : ils sont désormais difficiles à capter et restent réticents à tout changement.  Dans ce contexte les recruteurs ont compris qu'ils devraient faire des efforts pour attirer les meilleurs profils et semblent s'y adapter. \n\nMéthodologie :\nEtude de rémunération : données issues d'entretiens réalisés auprès de 50 000 candidats et clients dans le monde, de janvier à novembre 2022. Enquête Robert Walters : enquête réalisée auprès de plus de 1 700 cadres et entreprises interrogés en ligne en septembre 2022 en France.\nArticle rédigé par Véronique Arène, journaliste, sur “LE MONDE INFORMATIQUE”."
  },
  {
    "objectID": "Articles/Article_4.html",
    "href": "Articles/Article_4.html",
    "title": "Dégradation de la qualité et l’accessibilité des données en 2022",
    "section": "",
    "text": "D’après une étude publiée par l’éditeur Talend, la quasi-totalité des entreprises a éprouvé des difficultés à utiliser les données en 2022. En cause, une dégradation de la santé des données au cours de l’année passée, en particulier sur les aspects d’actualisation.\nPratiquement toutes les entreprises reconnaissent l’importance des données pour le succès de leurs stratégies, qu’il s’agisse d’augmenter le chiffre d’affaires, d’optimiser les coûts ou de réduire les risques. Pourtant, selon le baromètre 2022 sur la santé des données réalisé par l’éditeur Talend et la société Qualtrics, 97% des 892 décideurs et professionnels de la donnée interrogés témoignent de difficultés à utiliser les données dont ils disposent, leurs deux premiers défis étant d’assurer la qualité des données (49%) et d’y accéder rapidement (41%).\n\nÀ l’heure où l’agilité et la rapidité d’accès aux données peuvent faire la différence dans un contexte économique d’incertitude généralisée, près d’un sondé sur deux (46%) estime manquer de vitesse et de flexibilité dans ce domaine. Plus d’une entreprise sur quatre (41%) peine également à accéder rapidement aux données dont elle a besoin. L’enquête révèle par ailleurs une dégradation globale des cinq indicateurs de santé des données évalués entre 2021 et 2022 : l’actualisation chute de 18 points, la fiabilité, l’homogénéité et l’exhaustivité de 11 points et l’accessibilité de 9 points. Si malgré cela, 82% des répondants ont confiance dans leurs données, un écart notable sépare les professionnels IT (85% ont confiance) des métiers, qui ne sont que 75% à avoir confiance en celles-ci."
  },
  {
    "objectID": "Articles/Article_4.html#une-culture-data-encore-insuffisante",
    "href": "Articles/Article_4.html#une-culture-data-encore-insuffisante",
    "title": "Dégradation de la qualité et l’accessibilité des données en 2022",
    "section": "Une culture data encore insuffisante",
    "text": "Une culture data encore insuffisante\nParmi les facteurs expliquant la dégradation de la santé des données, les entreprises pointent notamment le travail à distance. Pour 57% des répondants, celui-ci accentue les difficultés d’accès à la donnée. Des enjeux d’ordre culturel transparaissent également, avec une organisation sur trois qui émet des réserves sur la compréhension des données par ses employés, faute en particulier de langage commun. Pour y remédier, 65% des entreprises interrogées ont lancé des programmes de data literacy (acculturation à la donnée).\nArticle rédigé par Aurélie Chandeze, rédactrice en chef adjointe de CIO, sur le “LE MONDE INFORMATIQUE”."
  }
]